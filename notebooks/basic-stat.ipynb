{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Basic User Stats\n",
    "Given a single user name and ID, we want to calculate all the basica stats and save it back to the DB. \n",
    "- Average CS per game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config('spark.driver.extraClassPath', './lib/postgresql-42.2.5.jar').getOrCreate()\n",
    "url = 'jdbc:postgresql://postgres:5432/lol'\n",
    "properties = {'user': 'lol', 'password': 'lol'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summoner_df = spark.read.jdbc(url=url, table=\"(select * from match limit 1) s\", properties=properties)\n",
    "summoner_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summoner_name = 'name_here'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting all matches for this summoner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_matches_query = f\"(select m.* from summoner s join matches map on s.id = map.summoner_id join match m on map.match_id = m.id where s.name = '{summoner_name}') a\"\n",
    "matches_df = spark.read.jdbc(url=url, table=get_all_matches_query, properties=properties)\n",
    "matches_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting `info` column with json type to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(spark.sparkContext)\n",
    "match_df = matches_df.first()\n",
    "info_df = sqlc.read.json(matches_df.rdd.map(lambda r: r.info))\n",
    "info_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out the participantId\n",
    "# pull participants->stats->goldEarned\n",
    "total_gold = 0\n",
    "total_match = 0\n",
    "total_average_gold_per_minute = 0\n",
    "for info in info_df.collect():\n",
    "    game_minutes = info.gameDuration/60\n",
    "    participant_id = 0\n",
    "    for participant in info.participantIdentities:\n",
    "        if participant.player.summonerName == summoner_name:\n",
    "            participant_id = participant.participantId\n",
    "    # found participantId\n",
    "    for p in info.participants: \n",
    "        if participant_id == p.participantId:\n",
    "            total_gold += p.stats.goldEarned\n",
    "            total_match += 1\n",
    "            average_gold_per_minute = p.stats.goldEarned / game_minutes\n",
    "            print(\"average_gold_per_minute: \", average_gold_per_minute)\n",
    "            total_average_gold_per_minute += average_gold_per_minute\n",
    "print(\"summoner: \", summoner_name)\n",
    "print(\"total_gold: \", total_gold)\n",
    "print(\"total_match: \", total_match)\n",
    "print(\"average_gold_per_match: \", total_gold/total_match)\n",
    "print(\"average_gold_per_minute: \", total_average_gold_per_minute / total_match)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from collections import OrderedDict\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "\n",
    "def convert_to_row(d: dict) -> Row:\n",
    "    return Row(**OrderedDict(sorted(d.items())))\n",
    "\n",
    "list = []\n",
    "for info in info_df.collect():\n",
    "    game_stat = {}\n",
    "    game_stat['gameDuration'] = info.gameDuration/60\n",
    "    participant_id = 0\n",
    "\n",
    "    for participant in info.participantIdentities:\n",
    "        if participant.player.summonerName == summoner_name:\n",
    "            participant_id = participant.participantId\n",
    "    # found participantId\n",
    "    for p in info.participants: \n",
    "        if participant_id == p.participantId:\n",
    "            game_stat['goldEarned'] = p.stats.goldEarned\n",
    "            game_stat['visionScore'] = p.stats.visionScore\n",
    "            game_stat['totalDamageDealt'] = p.stats.totalDamageDealt\n",
    "            game_stat['totalDamageTaken'] = p.stats.totalDamageTaken\n",
    "            game_stat['kills'] = p.stats.kills\n",
    "            game_stat['deaths'] = p.stats.deaths\n",
    "            game_stat['assists'] = p.stats.assists\n",
    "            game_stat['goldPerMinDeltas_10'] = p.timeline.goldPerMinDeltas.__getitem__(\"0-10\") if p.timeline.goldPerMinDeltas else 0            \n",
    "            game_stat['goldPerMinDeltas_20'] = p.timeline.goldPerMinDeltas.__getitem__('10-20') if p.timeline.goldPerMinDeltas else 0            \n",
    "            game_stat['goldPerMinDeltas_30'] = p.timeline.goldPerMinDeltas.__getitem__('20-30') if p.timeline.goldPerMinDeltas else 0\n",
    "            game_stat['creepsPerMinDeltas_10'] = p.timeline.creepsPerMinDeltas.__getitem__('0-10') if p.timeline.goldPerMinDeltas else 0\n",
    "            game_stat['creepsPerMinDeltas_20'] = p.timeline.creepsPerMinDeltas.__getitem__('10-20') if p.timeline.goldPerMinDeltas else 0\n",
    "            game_stat['creepsPerMinDeltas_30'] = p.timeline.creepsPerMinDeltas.__getitem__('20-30') if p.timeline.goldPerMinDeltas else 0\n",
    "            list.append(game_stat)\n",
    "\n",
    "df = spark.sparkContext.parallelize(list).map(convert_to_row).toDF()           \n",
    "new_df = df.select(avg(\"goldEarned\"), avg(\"visionScore\"), avg(\"totalDamageDealt\"), \n",
    "                   avg(\"totalDamageTaken\"), avg(\"kills\"), avg(\"deaths\"), avg(\"assists\"),\n",
    "                  avg(\"goldPerMinDeltas_10\"), avg(\"goldPerMinDeltas_20\"), avg(\"goldPerMinDeltas_30\"),\n",
    "                  avg(\"creepsPerMinDeltas_10\"), avg(\"creepsPerMinDeltas_20\"), avg(\"creepsPerMinDeltas_30\"))\n",
    "new_df.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"basic_stats\")\n",
    "spark.sql(\"select * from basic_stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
